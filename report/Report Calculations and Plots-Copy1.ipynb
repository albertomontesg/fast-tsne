{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from visualization import *\n",
    "from util_tsne_exact import *\n",
    "from sympy import *\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating FLOPs per part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iters_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-989b0e0ab62b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute the flops for baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miters_baseline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miters_baseline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\" cycles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iters_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "D = 28*28\n",
    "d = 2\n",
    "T = 1000\n",
    "\n",
    "def get_flops(iters, implementation):\n",
    "# Compute the flops for baseline\n",
    "N = iters_baseline[\"N\"]\n",
    "it = iters_baseline[\" cycles\"]\n",
    "\n",
    "flops_distance_baseline = 3*N*D + D + (1.5)*D*N*(N-1)\n",
    "flops_perplexity_baseline = it * (2 * N + N + 3 * N + 7) + N**2\n",
    "flops_symmetrize_baseline = N * (N-1) / 2 + 3 * N**2\n",
    "flops_ld_affinities_baseline = T * (7 * N * (N - 1) + 3 * N**2)\n",
    "flops_gradient_baseline = T * (9 * N * (N - 1) + 4*N + 2 + 2*N)\n",
    "flops_total_baseline = flops_distance_baseline + flops_perplexity_baseline + flops_symmetrize_baseline + \\\n",
    "    flops_ld_affinities_baseline + flops_gradient_baseline\n",
    "flops_baseline = np.vstack([N, flops_distance_baseline, flops_perplexity_baseline, \n",
    "                            flops_symmetrize_baseline, flops_ld_affinities_baseline,\n",
    "                           flops_gradient_baseline, flops_total_baseline]).T\n",
    "\n",
    "N = iters_scalar[\"N\"]\n",
    "it = iters_scalar[\" cycles\"]\n",
    "flops_distance_scalar = 3*N*D+ D + 2*N*D + (1.5)*D*N*(N-1)\n",
    "flops_perplexity_scalar = it * (2 * N + N + 3 * N + 7) + N**2\n",
    "flops_symmetrize_scalar = N * (N-1) / 2 + 3 * N**2\n",
    "flops_ld_affinities_scalar = T * (10/2 * N * (N - 1))\n",
    "flops_gradient_scalar = T * (9 * N**2 + 4*N + 2 + 2*N)\n",
    "flops_total_scalar = flops_distance_scalar + flops_perplexity_scalar + flops_perplexity_scalar + \\\n",
    "    flops_ld_affinities_scalar + flops_gradient_scalar\n",
    "flops_scalar = np.vstack([N, flops_distance_scalar, flops_perplexity_scalar,\n",
    "                         flops_symmetrize_scalar, flops_ld_affinities_scalar,\n",
    "                         flops_gradient_scalar, flops_total_scalar]).T\n",
    "\n",
    "N = iters_avx[\"N\"]\n",
    "it = iters_avx[\" cycles\"]\n",
    "flops_distance_avx = (D//8) * ( (N//8) * 64 + (N%8) * 8 + 7*8 + N*8 ) + \\\n",
    "        (d%8) * (8 * (N//8) + (N%8) + 7 +N ) + D + 2 +  1 + \\\n",
    "        (N//8) * (112*D + 448 + ( (N//8) -1) * (64*D + 256) + (N%8)*(2*D + 8) ) + \\\n",
    "        (N//8) * (56*D + 56 + ( (N//8) -1) * (32*D + 4) + (N%8)*(1*D + 1) )\n",
    "flops_perplexity_avx = it * (2 * N + N + 3 * N + 7) + N**2\n",
    "flops_symmetrize_avx = N * (N-1) / 2 + 3 * N**2\n",
    "flops_ld_affinities_avx = T * (N/8*(N/8-1)/2 * (16*8 + 8*8 + 8*8*2 + 8*8 + 8*8 + 8*8))\n",
    "flops_gradient_avx = T * (N/16*(N/8*(16*8 + 4*8*8 + 4*4*8 + 4*4*8 + 4*16*8) + (4*8 + 4*16)) + \\\n",
    "        N/(4*8) * (8*8) + 11*8 + N/(4*8)*(8*8))\n",
    "flops_total_avx = flops_distance_avx + flops_perplexity_avx + flops_symmetrize_avx + \\\n",
    "    flops_ld_affinities_avx + flops_gradient_avx\n",
    "flops_avx = np.vstack([N, flops_distance_avx, flops_perplexity_avx,\n",
    "                         flops_symmetrize_avx, flops_ld_affinities_avx,\n",
    "                         flops_gradient_avx, flops_total_avx]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it, D, N, d, T = symbols('it D N d T')\n",
    "count_measure = (\"add\", \"mult\", \"div\", \"exp\", \"log\")\n",
    "flops = {\n",
    "    \"normalize\": {\n",
    "        \"add\": 2 * N * D,\n",
    "        \"div\": D + N * D\n",
    "    },\n",
    "    \"compute_pairwise_affinity_perplexity\": {\n",
    "        \"compute_squared_euclidean_distance\": {\n",
    "            \"add\": D * N * (N - 1) / 2 * 2,\n",
    "            \"mult\": D * N * (N - 1) / 2\n",
    "        },\n",
    "        \"binary_search\": {\n",
    "            \"add\": it * (N + N + 1 + 1 + 1),\n",
    "            \"mult\": it * (N + 2 * N),\n",
    "            \"div\": it * (1 + 1) + N * N,\n",
    "            \"exp\": it * N,\n",
    "            \"log\": it * (1 + 1)\n",
    "        }\n",
    "    },\n",
    "    \"symmetrize_affinities\": {\n",
    "        \"add\": N * (N - 1) / 2 + N * N,\n",
    "        \"div\": N * N\n",
    "    },\n",
    "    \"early_exageration\": {\n",
    "        \"mult\": 2 * N * N\n",
    "    },\n",
    "    \"compute_low_dimensional_affinities\": {\n",
    "        \"compute_squared_euclidean_distance\": {\n",
    "            \"add\": T * d * N * (N - 1) / 2 * 2,\n",
    "            \"mult\": T * d * N * (N - 1) / 2\n",
    "        },\n",
    "        \"compute\": {\n",
    "            \"add\": T * N * (N - 1) * 2,\n",
    "            \"div\": T * N * (N - 1)\n",
    "        }\n",
    "    },\n",
    "    \"gradient_computation\": {\n",
    "        \"add\": T * N * (N - 1) * (1 + 2 * d),\n",
    "        \"mult\": T * N * (N - 1) * (1 + d),\n",
    "        \"div\": T * N * (N - 1)\n",
    "    },\n",
    "    \"gradient_update\": {\n",
    "        \"add\": T * (N * d + N * d * 2),\n",
    "        \"mult\": T * N * d * 3\n",
    "    },\n",
    "    \"normalize_2\": {\n",
    "        \"add\": T * 2 * N * d,\n",
    "        \"div\": T * (d + N * d)\n",
    "    }\n",
    "}\n",
    "\n",
    "flops_by_function_measure = dict(flops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1\n",
      "add D*N*(N + 1)\n",
      "mult D*N*(N - 1)/2\n",
      "div D*(N + 1)\n",
      "\n",
      "Part 2\n",
      "add 3*N**2/2 + 2*N*it - N/2 + 2*it - - N*(83*N + 79)/2\n",
      "mult N*(N + 3*it) - - 61*N**2\n",
      "div 2*N**2 + 2*it - - 2*N*(N + 20)\n",
      "exp N*it - - 20*N**2\n",
      "log 2*it - - 40*N\n",
      "\n",
      "Part 3\n",
      "add N*T*(5*d + (N - 1)*(2*d + 1))/2\n",
      "mult N*(2*N + 3*T*d + T*(N - 1)*(d + 1))/2\n",
      "div T*(N*(N - 1) + d*(N + 1))/2\n"
     ]
    }
   ],
   "source": [
    "cost_part_1 = {}\n",
    "cost_part_2 = {}\n",
    "cost_part_3 = {}\n",
    "\n",
    "for c in flops_by_function_measure[\"compute_pairwise_affinity_perplexity\"][\"compute_squared_euclidean_distance\"]:\n",
    "    if c not in cost_part_1:\n",
    "        cost_part_1[c] = 0\n",
    "    cost_part_1[c] += flops_by_function_measure[\"compute_pairwise_affinity_perplexity\"][\"compute_squared_euclidean_distance\"][c]\n",
    "    \n",
    "for c in flops_by_function_measure[\"normalize\"]:\n",
    "    if c not in cost_part_1:\n",
    "        cost_part_1[c] = 0\n",
    "    cost_part_1[c] += flops_by_function_measure[\"normalize\"][c]\n",
    "    \n",
    "    \n",
    "for c in flops_by_function_measure[\"compute_pairwise_affinity_perplexity\"][\"binary_search\"]:\n",
    "    if c not in cost_part_2:\n",
    "        cost_part_2[c] = 0\n",
    "    cost_part_2[c] += flops_by_function_measure[\"compute_pairwise_affinity_perplexity\"][\"binary_search\"][c]\n",
    "\n",
    "for c in flops_by_function_measure[\"symmetrize_affinities\"]:\n",
    "    if c not in cost_part_2:\n",
    "        cost_part_2[c] = 0\n",
    "    cost_part_2[c] += flops_by_function_measure[\"symmetrize_affinities\"][c]\n",
    "    \n",
    "    \n",
    "for c in flops_by_function_measure[\"early_exageration\"]:\n",
    "    if c not in cost_part_2:\n",
    "        cost_part_2[c] = 0\n",
    "    cost_part_2[c] += flops_by_function_measure[\"early_exageration\"][c]/2\n",
    "\n",
    "    \n",
    "for t in [\"gradient_computation\", \"gradient_update\", \"normalize_2\"]:\n",
    "    for c in flops_by_function_measure[t]:\n",
    "        if c not in cost_part_3:\n",
    "            cost_part_3[c] = 0\n",
    "        cost_part_3[c] += flops_by_function_measure[t][c]/2\n",
    "\n",
    "        \n",
    "for c in flops_by_function_measure[\"early_exageration\"]:\n",
    "    if c not in cost_part_3:\n",
    "        cost_part_3[c] = 0\n",
    "    cost_part_3[c] += flops_by_function_measure[\"early_exageration\"][c]/2\n",
    "\n",
    "        \n",
    "print(\"Part 1\")\n",
    "for c in cost_part_1:\n",
    "    print(c, cost_part_1[c].simplify())\n",
    "\n",
    "print()\n",
    "print(\"Part 2\")\n",
    "for c in cost_part_2:\n",
    "    print(c, cost_part_2[c].simplify(), \"- -\", cost_part_2[c].subs('it', 20*N).simplify())\n",
    "    \n",
    "print()\n",
    "print(\"Part 3\")\n",
    "for c in cost_part_3:\n",
    "    print(c, cost_part_3[c].simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO use a combined cost measure where div's are more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time spend per part Based on D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d500 = pd.read_csv(\"../code/implementations/tsne_exact_final/toaster_bench/D500\")\n",
    "d1000 = pd.read_csv(\"../code/implementations/tsne_exact_final/toaster_bench/D1000\")\n",
    "d2500 = pd.read_csv(\"../code/implementations/tsne_exact_final/toaster_bench/D2500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0448644654934 0.180571933685 0.774563600821\n"
     ]
    }
   ],
   "source": [
    "part1_500 = d500[\"pairwise_squared_euclidean_distance\"]\n",
    "part2_500 = d500[\"pairwise_affinity_perplexity\"] + d500[\"symmetrize_affinities\"]\n",
    "part3_500 = d500[\"low_dimensional_affinities\"] + d500[\"gradient_computation_update_normalize\"]\n",
    "tot = d500[\"pairwise_squared_euclidean_distance\"] + d500[\"pairwise_affinity_perplexity\"] + d500[\"symmetrize_affinities\"] + d500[\"low_dimensional_affinities\"] + d500[\"gradient_computation_update_normalize\"]\n",
    "part1_500 /= tot\n",
    "part2_500 /= tot\n",
    "part3_500 /= tot\n",
    "part1_500 = part1_500[0]\n",
    "part2_500 = part2_500[0]\n",
    "part3_500 = part3_500[0]\n",
    "print(part1_500, part2_500, part3_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0884400224532 0.172425723104 0.739134254443\n"
     ]
    }
   ],
   "source": [
    "part1_1000 = d1000[\"pairwise_squared_euclidean_distance\"]\n",
    "part2_1000 = d1000[\"pairwise_affinity_perplexity\"] + d1000[\"symmetrize_affinities\"]\n",
    "part3_1000 = d1000[\"low_dimensional_affinities\"] + d1000[\"gradient_computation_update_normalize\"]\n",
    "tot = d1000[\"pairwise_squared_euclidean_distance\"] + d1000[\"pairwise_affinity_perplexity\"] + d1000[\"symmetrize_affinities\"] + d1000[\"low_dimensional_affinities\"] + d1000[\"gradient_computation_update_normalize\"]\n",
    "part1_1000 /= tot\n",
    "part2_1000 /= tot\n",
    "part3_1000 /= tot\n",
    "part1_1000 = part1_1000[0]\n",
    "part2_1000 = part2_1000[0]\n",
    "part3_1000 = part3_1000[0]\n",
    "print(part1_1000, part2_1000, part3_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.233251350304 0.172769192406 0.59397945729\n"
     ]
    }
   ],
   "source": [
    "part1_2500 = d2500[\"pairwise_squared_euclidean_distance\"]\n",
    "part2_2500 = d2500[\"pairwise_affinity_perplexity\"] + d2500[\"symmetrize_affinities\"]\n",
    "part3_2500 = d2500[\"low_dimensional_affinities\"] + d2500[\"gradient_computation_update_normalize\"]\n",
    "tot = d2500[\"pairwise_squared_euclidean_distance\"] + d2500[\"pairwise_affinity_perplexity\"] + d2500[\"symmetrize_affinities\"] + d2500[\"low_dimensional_affinities\"] + d2500[\"gradient_computation_update_normalize\"]\n",
    "part1_2500 /= tot\n",
    "part2_2500 /= tot\n",
    "part3_2500 /= tot\n",
    "part1_2500 = part1_2500[0]\n",
    "part2_2500 = part2_2500[0]\n",
    "part3_2500 = part3_2500[0]\n",
    "print(part1_2500, part2_2500, part3_2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>pairwise_squared_euclidean_distance</th>\n",
       "      <th>pairwise_affinity_perplexity</th>\n",
       "      <th>symmetrize_affinities</th>\n",
       "      <th>low_dimensional_affinities</th>\n",
       "      <th>gradient_computation_update_normalize</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>5.039704e+09</td>\n",
       "      <td>3.723792e+09</td>\n",
       "      <td>9.115551e+06</td>\n",
       "      <td>8.740625e+09</td>\n",
       "      <td>4.093088e+09</td>\n",
       "      <td>4.318419e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      N  pairwise_squared_euclidean_distance  pairwise_affinity_perplexity  \\\n",
       "0  1000                         5.039704e+09                  3.723792e+09   \n",
       "\n",
       "   symmetrize_affinities  low_dimensional_affinities  \\\n",
       "0           9.115551e+06                8.740625e+09   \n",
       "\n",
       "   gradient_computation_update_normalize         total  \n",
       "0                           4.093088e+09  4.318419e+10  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
